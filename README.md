I developed a word-level text generation model using Long Short-Term Memory (LSTM) networks in an unsupervised learning setting. A raw text file was used as the dataset, which was preprocessed by cleaning, tokenizing, and splitting into sequences of words. Each sequence was structured so that a group of preceding words was used as input to predict the next word, allowing the model to learn contextual relationships and language patterns. The LSTM model was trained to capture these sequential dependencies and generate coherent and meaningful text. This project helped deepen my understanding of sequential modeling and natural language generation techniques.  
